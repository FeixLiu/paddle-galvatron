# Qwen

## 1. Model Introduction

[Qwen](https://arxiv.org/abs/2205.01068) is a series of large language models developed by Alibaba Cloud, available in 7B and 14B parameter sizes. Based on the Transformer architecture, Qwen is trained on massive and diverse pre-training data including web text, professional books, and code.

**Supported Model Weights:**
| Model              |
|--------------------|
| qwen/qwen-7b       |
| qwen/qwen-7b-chat  |
| qwen/qwen-14b      |
| qwen/qwen-14b-chat |
| qwen/qwen-72b      |
| qwen/qwen-72b-chat |

[Qwen1.5](https://qwenlm.github.io/blog/qwen1.5/) is an upgraded version of the Qwen series, including 0.5B, 1.8B, 4B, 7B, 14B, 32B, 72B, 110B and MoE variants, with both Base and Chat versions.

**Supported Model Weights:**
| Model (qwen-1.5)            |
|-----------------------------|
| Qwen/Qwen1.5-0.5B           |
| Qwen/Qwen1.5-0.5B-Chat      |
| Qwen/Qwen1.5-1.8B           |
| Qwen/Qwen1.5-1.8B-Chat      |
| Qwen/Qwen1.5-4B             |
| Qwen/Qwen1.5-4B-Chat        |
| Qwen/Qwen1.5-7B             |
| Qwen/Qwen1.5-7B-Chat        |
| Qwen/Qwen1.5-14B            |
| Qwen/Qwen1.5-14B-Chat       |
| Qwen/Qwen1.5-32B            |
| Qwen/Qwen1.5-32B-Chat       |
| Qwen/Qwen1.5-72B            |
| Qwen/Qwen1.5-72B-Chat       |
| Qwen/Qwen1.5-110B           |
| Qwen/Qwen1.5-110B-Chat      |
| Qwen/Qwen1.5-MoE-A2.7B      |
| Qwen/Qwen1.5-MoE-A2.7B-Chat |

[Qwen2](https://qwenlm.github.io/blog/qwen2/) is the latest iteration of the Qwen series, offering 0.5B, 1.5B, 7B, 72B and MoE variants with Base and Chat versions.

**Supported Model Weights:**
| Model (qwen2)                |
|------------------------------|
| Qwen/Qwen2-0.5B              |
| Qwen/Qwen2-0.5B-Instruct     |
| Qwen/Qwen2-1.5B              |
| Qwen/Qwen2-1.5B-Instruct     |
| Qwen/Qwen2-7B                |
| Qwen/Qwen2-7B-Instruct       |
| Qwen/Qwen2-72B               |
| Qwen/Qwen2-72B-Instruct      |
| Qwen/Qwen2-57B-A14B          |
| Qwen/Qwen2-57B-A14B-Instruct |

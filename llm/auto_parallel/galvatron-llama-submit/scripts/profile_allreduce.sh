export NCCL_IB_HCA=mlx5_bond_1,mlx5_bond_4,mlx5_bond_3,mlx5_bond_2,mlx5_bond_7,mlx5_bond_6,mlx5_bond_8,mlx5_bond_5
export NCCL_IB_DISABLE=0
echo "Running /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir output/profile_allreduce /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_allreduce.py  --output_dir "./output" --profile_time 0 --tp_deg 8 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/allreduce_bandwidth_1nodes_8gpus_per_node.json "
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir output/profile_allreduce /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_allreduce.py  --output_dir "./output" --profile_time 0 --tp_deg 8 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/allreduce_bandwidth_1nodes_8gpus_per_node.json 
sleep 1
echo "Running /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir output/profile_allreduce /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_allreduce.py  --output_dir "./output" --profile_time 0 --tp_deg 4 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/allreduce_bandwidth_1nodes_8gpus_per_node.json "
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir output/profile_allreduce /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_allreduce.py  --output_dir "./output" --profile_time 0 --tp_deg 4 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/allreduce_bandwidth_1nodes_8gpus_per_node.json 
sleep 1
echo "Running /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir output/profile_allreduce /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_allreduce.py  --output_dir "./output" --profile_time 0 --tp_deg 2 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/allreduce_bandwidth_1nodes_8gpus_per_node.json "
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir output/profile_allreduce /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_allreduce.py  --output_dir "./output" --profile_time 0 --tp_deg 2 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/allreduce_bandwidth_1nodes_8gpus_per_node.json 
sleep 1
rm -r ./profiler_log
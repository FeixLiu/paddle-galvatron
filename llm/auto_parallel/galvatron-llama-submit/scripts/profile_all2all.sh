export NCCL_IB_HCA=mlx5_bond_1,mlx5_bond_4,mlx5_bond_3,mlx5_bond_2,mlx5_bond_7,mlx5_bond_6,mlx5_bond_8,mlx5_bond_5
export NCCL_IB_DISABLE=0
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 8 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 1024"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 8 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 1024
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 8 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 512"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 8 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 512
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 8 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 256"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 8 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 256
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 8 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 128"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 8 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 128
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 8 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 64"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 8 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 64
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 8 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 32"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 8 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 32
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 8 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 16"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 8 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 16
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 8 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 8"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 8 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 8
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 8 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 4"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 8 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 4
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 8 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 2"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 8 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 2
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 8 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 1"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 8 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 1
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 4 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 1024"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 4 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 1024
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 4 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 512"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 4 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 512
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 4 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 256"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 4 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 256
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 4 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 128"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 4 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 128
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 4 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 64"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 4 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 64
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 4 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 32"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 4 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 32
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 4 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 16"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 4 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 16
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 4 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 8"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 4 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 8
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 4 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 4"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 4 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 4
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 4 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 2"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 4 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 2
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 4 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 1"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 4 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 1
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 2 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 1024"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 2 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 1024
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 2 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 512"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 2 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 512
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 2 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 256"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 2 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 256
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 2 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 128"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 2 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 128
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 2 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 64"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 2 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 64
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 2 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 32"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 2 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 32
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 2 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 16"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 2 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 16
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 2 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 8"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 2 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 8
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 2 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 4"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 2 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 4
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 2 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 2"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 2 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 2
sleep 1
echo "Running: /apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 2 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 1"
/apdcephfs_fsgm/share_303760348/anaconda3/envs/lgm-paddle/bin/python3 -u -m paddle.distributed.launch --ips 28.12.131.41 --gpus 0,1,2,3,4,5,6,7 --log_dir /output/profile_all2all /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/paddlenlp/experimental/galvatron/profiler/profile_all2all.py  --output_dir "./output" --tp_deg 2 --save_file_name /apdcephfs_fsgm/share_303760348/guangming/WorkSpace/paddle3.0/llm/auto_parallel/galvatron-llama-submit/configs/sp_time_1nodes_8gpus_per_node.json --local_batch_size 1
sleep 1
